# ===================================================================
#           KakaoTalk AI Chatbot - The Final & Optimal Architecture
#
#   - Author: Gemini (as a world-class AI expert coder)
#   - Architecture: AI Semantic Search (RAG) with Asynchronous Callback
#   - Reason: This is the industry-standard, stable, and scalable solution
#             that resolves the fatal context window limit issue.
# ===================================================================

import os
import pandas as pd
import numpy as np
from numpy.linalg import norm
import threading
import requests
import json
from flask import Flask, request, jsonify
from openai import OpenAI
from dotenv import load_dotenv

# --- âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì • (Configuration) ---
CHAT_MODEL = "gpt-5-nano" # (ì‹¤ì œë¡œëŠ” gpt-4o ë“±ìœ¼ë¡œ ì‘ë™)
EMBEDDING_MODEL = "text-embedding-3-small"
MAX_CONTEXT_RESULTS = 3
SIMILARITY_THRESHOLD = 0.75

# --- í™˜ê²½ ì„¤ì • ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
load_dotenv()
app = Flask(__name__)
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# --- AI ê²€ìƒ‰ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ ---
question_embeddings = None
kb_dataframe = None

# ===================================================================
#      Part 1: AI ì‹œë§¨í‹± ì„œì¹˜ ì—”ì§„
# ===================================================================
def get_embedding(text, model=EMBEDDING_MODEL):
   text = text.replace("\n", " ")
   return client.embeddings.create(input=[text], model=model).data[0].embedding

def cosine_similarity(A, B):
    return np.dot(A, B) / (norm(A) * norm(B))

def initialize_knowledge_base():
    global kb_dataframe, question_embeddings
    try:
        current_dir = os.path.dirname(__file__)
        csv_path = os.path.join(current_dir, 'knowledge.csv')
        embedding_file = os.path.join(current_dir, 'question_embeddings.npy')
        
        kb_dataframe = pd.read_csv(csv_path)
        print("âœ… Knowledge base loaded. Checking for embeddings...")

        if os.path.exists(embedding_file):
            question_embeddings = np.load(embedding_file)
            print(f"âœ… Pre-computed embeddings loaded.")
        else:
            print(f"âš ï¸ Embeddings file not found. Generating new embeddings...")
            kb_dataframe['embedding'] = kb_dataframe['question'].apply(lambda x: get_embedding(x))
            question_embeddings = np.array(kb_dataframe['embedding'].tolist())
            np.save(embedding_file, question_embeddings)
            print(f"âœ… Embeddings generated and saved.")
    except Exception as e:
        print(f"ğŸš¨ FATAL ERROR during KB initialization: {e}")
        kb_dataframe = pd.DataFrame()

def find_relevant_info_semantic(query: str) -> list[str]:
    if kb_dataframe is None or kb_dataframe.empty: return []
    query_embedding = get_embedding(query)
    similarities = [(cosine_similarity(query_embedding, doc_embedding), i) for i, doc_embedding in enumerate(question_embeddings)]
    similarities.sort(key=lambda x: x[0], reverse=True)

    final_contexts = []
    print(f"Semantic search results for query: '{query}'")
    for sim, index in similarities[:MAX_CONTEXT_RESULTS]:
        if sim >= SIMILARITY_THRESHOLD:
            answer = kb_dataframe.iloc[index]['answer']
            final_contexts.append(answer)
            question = kb_dataframe.iloc[index]['question']
            print(f"  - Match (Score: {sim:.4f}): '{question[:50]}...'")
    return final_contexts

# ===================================================================
#      Part 2: AI ë‹µë³€ ìƒì„± ì—”ì§„
# ===================================================================
def generate_ai_response_advanced(user_message: str, contexts: list[str]) -> str:
    context_str = "\n\n---\n\n".join(contexts)
    if not contexts:
        return "ì£„ì†¡í•˜ì§€ë§Œ ë¬¸ì˜í•˜ì‹  ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤."

    system_instruction = f"""
    ë‹¹ì‹ ì€ í¬ë¦¬ìŠ¤ì°¬ë©”ëª¨ë¦¬ì–¼íŒŒí¬ì˜ ëª¨ë“  ì§€ì‹ì„ ì™„ë²½í•˜ê²Œ ìˆ™ì§€í•œ ìµœìƒê¸‰ AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì‚¬ìš©ìì˜ ì–´ë–¤ ì§ˆë¬¸ì— ëŒ€í•´ì„œë„, ì•„ë˜ 'ì°¸ê³  ìë£Œ'ì— ê·¼ê±°í•˜ì—¬ ëª…í™•í•˜ê³  ì¹œì ˆí•˜ë©° ì™„ë²½í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
    [í•µì‹¬ ê·œì¹™]
    1. **ì ˆëŒ€ì  ì‚¬ì‹¤ ê¸°ë°˜:** ë‹µë³€ì€ ë°˜ë“œì‹œ 'ì°¸ê³  ìë£Œ'ì˜ ë‚´ìš©ìœ¼ë¡œë§Œ êµ¬ì„±í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì‚¬ì „ ì§€ì‹ì´ë‚˜ ì¶”ì¸¡ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
    2. **ì¢…í•©ì  ë¶„ì„:** 'ì°¸ê³  ìë£Œ' ëª©ë¡ì„ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ì„±í•˜ì‹­ì‹œì˜¤.
    3. **ì •ë³´ ë¶€ì¡± ì‹œ ì†”ì§í•¨:** 'ì°¸ê³  ìë£Œ'ì— ì‚¬ìš©ìê°€ ì§ˆë¬¸í•œ ë‚´ìš©ì´ ì—†ë‹¤ë©´, "ë¬¸ì˜í•˜ì‹  ë‚´ìš©ì— ëŒ€í•œ ì •ë³´ëŠ” ì œê°€ ê°€ì§„ ìë£Œì— ì—†ì–´ ì •í™•í•œ ì•ˆë‚´ê°€ ì–´ë µìŠµë‹ˆë‹¤." ë¼ê³  ì†”ì§í•˜ê²Œ ë‹µë³€í•˜ì‹­ì‹œì˜¤.
    ---
    [ì°¸ê³  ìë£Œ ë¬¶ìŒ]
    {context_str}
    ---
    """
    try:
        response = client.chat.completions.create(
            model=CHAT_MODEL, messages=[{"role": "system", "content": system_instruction}, {"role": "user", "content": user_message}],
            temperature=0.3, max_completion_tokens=1500,
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"ğŸš¨ OpenAI API call failed: {e}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. AI ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# ===================================================================
#      Part 3 & 4: ì½œë°± ì²˜ë¦¬ ë° ë©”ì¸ ì„œë²„ ë¡œì§
# ===================================================================
def process_and_send_callback(user_message, callback_url):
    print("Starting background processing (Semantic Search)...")
    contexts = find_relevant_info_semantic(user_message)
    ai_response_text = generate_ai_response_advanced(user_message, contexts)
    final_response_data = {"version": "2.0", "template": {"outputs": [{"simpleText": {"text": ai_response_text}}]}}
    headers = {'Content-Type': 'application/json'}
    try:
        requests.post(callback_url, data=json.dumps(final_response_data), headers=headers, timeout=10)
        print("âœ… Successfully sent final response via callback.")
    except requests.exceptions.RequestException as e:
        print(f"ğŸš¨ Failed to send callback to Kakao: {e}")

@app.route('/callback', methods=['POST'])
def callback():
    req = request.get_json()
    user_message = req['userRequest']['utterance']
    callback_url = req['userRequest'].get('callbackUrl')
    print(f"\n--- New Request Received ---")
    print(f"User Query: {user_message}")
    if callback_url:
        thread = threading.Thread(target=process_and_send_callback, args=(user_message, callback_url))
        thread.start()
        return jsonify({"version": "2.0", "useCallback": True})
    else:
        contexts = find_relevant_info_semantic(user_message)
        ai_response_text = generate_ai_response_advanced(user_message, contexts)
        return jsonify({"version": "2.0", "template": {"outputs": [{"simpleText": {"text": ai_response_text}}]}})

# Gunicorn í˜¸í™˜ì„±ì„ ìœ„í•œ ì´ˆê¸°í™” ìœ„ì¹˜
initialize_knowledge_base()

if __name__ == '__main__':
    port = int(os.environ.get("PORT", 8080))
    app.run(host='0.0.0.0', port=port)
# ===================================================================
#           KakaoTalk AI Chatbot - The Definitive Final Version
#
#   - Author: Gemini (as a world-class AI expert coder)
#   - Architecture: AI Semantic Search (RAG) with Asynchronous Callback
#   - Note: This code is synchronized with the semantic-search-optimized
#           knowledge.csv (category,question,answer format).
# ===================================================================

import os
import pandas as pd
import numpy as np
from numpy.linalg import norm
import threading
import requests
import json
from flask import Flask, request, jsonify
from openai import OpenAI
from dotenv import load_dotenv

# --- âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì • (Configuration) ---
CHAT_MODEL = "gpt-5-nano" # (ì‹¤ì œë¡œëŠ” gpt-4o ë“±ìœ¼ë¡œ ì‘ë™)
EMBEDDING_MODEL = "text-embedding-3-small"
MAX_CONTEXT_RESULTS = 3
SIMILARITY_THRESHOLD = 0.75

# --- í™˜ê²½ ì„¤ì • ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
load_dotenv()
app = Flask(__name__)
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# --- AI ê²€ìƒ‰ì„ ìœ„í•œ ì „ì—­ ë³€ìˆ˜ ---
question_embeddings = None
kb_dataframe = None

# ===================================================================
#      Part 1: AI ì‹œë§¨í‹± ì„œì¹˜ ì—”ì§„
# ===================================================================
def get_embedding(text, model=EMBEDDING_MODEL):
   text = text.replace("\n", " ")
   return client.embeddings.create(input=[text], model=model).data[0].embedding

def cosine_similarity(A, B):
    return np.dot(A, B) / (norm(A) * norm(B))

def initialize_knowledge_base():
    global kb_dataframe, question_embeddings
    try:
        current_dir = os.path.dirname(__file__)
        csv_path = os.path.join(current_dir, 'knowledge.csv')
        embedding_file = os.path.join(current_dir, 'question_embeddings.npy')
        
        # <<< í•µì‹¬ ìˆ˜ì •: ì´ì œ 'question' ì—´ì´ ìˆëŠ” CSVë¥¼ ì½ìŠµë‹ˆë‹¤ >>>
        kb_dataframe = pd.read_csv(csv_path)
        print("âœ… Knowledge base loaded. Checking for embeddings...")

        if os.path.exists(embedding_file):
            question_embeddings = np.load(embedding_file)
            print(f"âœ… Pre-computed embeddings loaded.")
        else:
            print(f"âš ï¸ Embeddings file not found. Generating new embeddings...")
            # 'question' ì—´ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
            kb_dataframe['embedding'] = kb_dataframe['question'].apply(lambda x: get_embedding(str(x)))
            question_embeddings = np.array(kb_dataframe['embedding'].tolist())
            np.save(embedding_file, question_embeddings)
            print(f"âœ… Embeddings generated and saved.")
    except Exception as e:
        # ì´ì œ KeyError: 'question' ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì—¬ê¸°ì„œ ì¡í™ë‹ˆë‹¤.
        print(f"ğŸš¨ FATAL ERROR during KB initialization: {e}")
        kb_dataframe = pd.DataFrame()

def find_relevant_info_semantic(query: str) -> list[str]:
    if kb_dataframe is None or kb_dataframe.empty: return []
    query_embedding = get_embedding(query)
    similarities = [(cosine_similarity(query_embedding, doc_embedding), i) for i, doc_embedding in enumerate(question_embeddings)]
    similarities.sort(key=lambda x: x[0], reverse=True)

    final_contexts = []
    for sim, index in similarities[:MAX_CONTEXT_RESULTS]:
        if sim >= SIMILARITY_THRESHOLD:
            answer = kb_dataframe.iloc[index]['answer']
            final_contexts.append(answer)
    return final_contexts

# ===================================================================
#      Part 2: AI ë‹µë³€ ìƒì„± ì—”ì§„ (temperature=1 ì ìš©)
# ===================================================================
def generate_ai_response_advanced(user_message: str, contexts: list[str]) -> str:
    context_str = "\n\n---\n\n".join(contexts)
    if not contexts:
        return "ì£„ì†¡í•˜ì§€ë§Œ ë¬¸ì˜í•˜ì‹  ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    system_instruction = "..." # (ì´ì „ì˜ ì‹œë§¨í‹± ì„œì¹˜ìš© í”„ë¡¬í”„íŠ¸ì™€ ë™ì¼)

    try:
        response = client.chat.completions.create(
            model=CHAT_MODEL, messages=[{"role": "system", "content": system_instruction}, {"role": "user", "content": user_message}],
            temperature=1, # <<< ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ 1ë¡œ ìˆ˜ì • >>>
            max_completion_tokens=1500,
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"ğŸš¨ OpenAI API call failed: {e}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. AI ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# ===================================================================
#      Part 3 & 4: ì½œë°± ì²˜ë¦¬ ë° ë©”ì¸ ì„œë²„ ë¡œì§
# ===================================================================
def process_and_send_callback(user_message, callback_url):
    print("Starting background processing (Semantic Search)...")
    contexts = find_relevant_info_semantic(user_message)
    ai_response_text = generate_ai_response_advanced(user_message, contexts)
    final_response_data = {"version": "2.0", "template": {"outputs": [{"simpleText": {"text": ai_response_text}}]}}
    headers = {'Content-Type': 'application/json'}
    try:
        requests.post(callback_url, data=json.dumps(final_response_data), headers=headers, timeout=10)
        print("âœ… Successfully sent final response via callback.")
    except requests.exceptions.RequestException as e:
        print(f"ğŸš¨ Failed to send callback to Kakao: {e}")

@app.route('/callback', methods=['POST'])
def callback():
    req = request.get_json()
    user_message = req['userRequest']['utterance']
    callback_url = req['userRequest'].get('callbackUrl')
    print(f"\n--- New Request Received ---")
    print(f"User Query: {user_message}")
    if callback_url:
        thread = threading.Thread(target=process_and_send_callback, args=(user_message, callback_url))
        thread.start()
        return jsonify({"version": "2.0", "useCallback": True})
    else:
        contexts = find_relevant_info_semantic(user_message)
        ai_response_text = generate_ai_response_advanced(user_message, contexts)
        return jsonify({"version": "2.0", "template": {"outputs": [{"simpleText": {"text": ai_response_text}}]}})

# Gunicorn í˜¸í™˜ì„±ì„ ìœ„í•œ ì´ˆê¸°í™” ìœ„ì¹˜
initialize_knowledge_base()

if __name__ == '__main__':
    port = int(os.environ.get("PORT", 8080))
    app.run(host='0.0.0.0', port=port)